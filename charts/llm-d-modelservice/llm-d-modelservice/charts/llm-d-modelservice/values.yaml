# yaml-language-server: $schema=values.schema.json
# This values.yaml file is a default values file. It can be overridden using another.

# @schema
# additionalProperties: true
# @schema
# -- Common configuration values
common: {}
# -- Usually used when using llm-d-modelservice as a subchart
enabled: true
# -- String to fully override common.names.fullname
fullnameOverride: ""
# -- Override to default pod serviceAccountName
serviceAccountOverride: ""
# schedulerName -- Name of the scheduler to use for scheduling model pods
# schedulerName: default-scheduler

# Global configuration that can be referenced by subcharts and components
global:
  # OpenTelemetry distributed tracing configuration
  # When enabled, vLLM containers will export traces to the OTLP endpoint
  tracing:
    # -- Enable distributed tracing for vLLM containers
    enabled: false
    # -- OTLP gRPC endpoint for trace export (e.g., OpenTelemetry Collector)
    # Override via: export OTEL_COLLECTOR_ENDPOINT="http://your-collector:4317"
    otlpEndpoint: "http://opentelemetry-collector.monitoring.svc.cluster.local:4317"
    # -- Trace sampling configuration
    sampling:
      # -- OpenTelemetry sampler type (e.g., "parentbased_traceidratio", "always_on", "always_off")
      sampler: "parentbased_traceidratio"
      # -- Sampling ratio (0.0 to 1.0). Use 1.0 for 100% sampling (demo/debug), 0.1 for 10% (production)
      samplerArg: "1.0"
    # -- Service name overrides for different components
    serviceNames:
      # -- Service name for vLLM decode instances
      vllmDecode: "vllm-decode"
      # -- Service name for vLLM prefill instances
      vllmPrefill: "vllm-prefill"
    # -- vLLM-specific tracing options
    vllm:
      # -- Level of trace detail collection (e.g., "all", "model", "scheduler")
      collectDetailedTraces: "all"
modelArtifacts:
  # name is the value of the model parameter in OpenAI requests
  # Required
  name: random/model
  # Labels that will be added to the pods serving the model.
  # These should match the labels of any associated InferencePool
  # In addition, the label llm-d.ai/role will be added with the value 'prefill'
  # or 'decode' depending on the role of the pod.
  # @schema
  # type: object
  # additionalProperties: true
  # @schema
  labels:
    llm-d.ai/inference-serving: "true"
  # model URI. One of:
  #   hf://model/name - model as defined on Hugging Face
  #   pvc://pvc_name/path/to/model - model on existing persistant storage volume
  #   oci:// not yet supported
  uri: "hf://{{ .Values.modelArtifacts.name }}"
  # size of volume to create to hold the model
  size: 5Mi
  # name of secret containing credentials for accessing the model (e.g., HF_TOKEN)
  authSecretName: ""
  # location where model volume will be mounted (used when mountModelVolume: true)
  mountPath: /model-cache
# When true, a LeaderWorkerSet is used instead of a Deployment
multinode: false
# Global accelerator configuration
# Supported types: nvidia, intel-i915, intel-xe, intel-gaudi, amd, google, cpu
accelerator:
  # Type of accelerator to use
  type: nvidia
  # @schema
  # type: boolean
  # @schema
  # Enable Dynamic Resource Allocation (DRA) for accelerators
  # When true, uses resourceClaimTemplates instead of device plugin resources
  dra: false
  # Resource names for different accelerator types (used when dra: false)
  resources:
    nvidia: "nvidia.com/gpu"
    intel-i915: "gpu.intel.com/i915"
    intel-xe: "gpu.intel.com/xe"
    intel-gaudi: "habana.ai/gaudi"
    amd: "amd.com/gpu"
    google: "google.com/tpu"
  # Environment variables specific to accelerator types
  env:
    intel-i915:
      - name: VLLM_USE_V1
        value: "1"
      - name: TORCH_LLM_ALLREDUCE
        value: "1"
      - name: VLLM_WORKER_MULTIPROC_METHOD
        value: "spawn"
    intel-xe:
      - name: VLLM_WORKER_MULTIPROC_METHOD
        value: "spawn"
  # ResourceClaimTemplate configurations for DRA (used when dra: true)
  # Each accelerator type can have its own claim template configuration
  resourceClaimTemplates:
    nvidia:
      name: nvidia-claim-template
      class: gpu.nvidia.com
      match: "exactly"
      # count: 1  # Optional: If not specified, auto-calculated from parallelism (tensor * dataLocal)
      selectors: []
    intel-i915:
      name: intel-i915-claim-template
      class: gpu.intel.com
      match: "exactly"
      selectors: []
    intel-xe:
      name: intel-xe-claim-template
      class: gpu.intel.com
      match: "exactly"
      selectors: []
    intel-gaudi:
      name: intel-gaudi-claim-template
      class: gaudi.intel.com
      match: "exactly"
      selectors: []
    amd:
      name: amd-claim-template
      class: gpu.amd.com
      match: "exactly"
      selectors: []
    google:
      name: google-claim-template
      class: tpu.google.com
      match: "exactly"
      selectors: []
# @schema
# additionalProperties: true
# @schema
# -- Requester configuration part of the dual-pod solution for FMA
requester:
  enable: false
  image:
    registry: ghcr.m.daocloud.io
    repository: llm-d-incubation/llm-d-fast-model-actuation/requester
    tag: latest
  adminPort: 8081
  port:
    probes: 8080
    spi: 8081
  readinessProbe:
    initialDelaySeconds: 2
    periodSeconds: 5
  resources:
    limits:
      gpus: 1
      cpus: 1
      memory: 250Mi
# Describe routing requirements. In addition to service level routing (OpenAI model name, service port)
# also describes elements for Gateway API Inference Extension configuration
routing:
  # Deprecated
  # modelName: random/modelId
  # port the inference engine (vLLM) listens
  # when a sidecar (proxy) is used it will listen on this port and forward to VLLM on proxy.targetPort
  servicePort: 8000
  # @schema
  # additionalProperties: true
  # @schema
  # Configuration of VLLM routing sidecar
  # cf. https://github.com/llm-d/llm-d-routing-sidecar/
  proxy:
    enabled: true
    image:
      registry: ghcr.m.daocloud.io
      repository: llm-d/llm-d-routing-sidecar
      tag: latest
    imagePullPolicy: Always
    # target port on which VLLM should listen
    targetPort: 8200
    # Specify a conenctor. For example, `nixl`, `nixlv2`
    connector: nixlv2
    # Boolean: adds the `--secure-proxy` flag to the routingSidecar with your chosen value.  Arg is ommitted by default for compatability with legacy sidecar images.
    secure: false
    # Boolean: whether to use TLS when sending requests to prefillers. Arg is ommitted by default for compatability with legacy sidecar images.
    # prefillerUseTLS: true

    # The path to the certificate for secure proxy.  Arg is ommitted by default for compatability with legacy sidecar images.
    # certPath: "/certs"

    # Zap logging configuration
    # Boolean: Development Mode defaults(encoder=consoleEncoder,logLevel=Debug,stackTraceLevel=Warn). Production Mode defaults(encoder=jsonEncoder,logLevel=Info,stackTraceLevel=Error)
    # @schema
    # type: boolean
    # default:false
    # @schema
    # zapDevel: false

    # String: Zap log encoding (one of 'json' or 'console')
    # @schema
    # default: json
    # enum: [json,console]
    # @schema
    zapEncoder: json
    # String or integer: Zap Level to configure the verbosity of logging. Can be one of 'debug', 'info', 'error', 'panic' or any integer value > 0 which corresponds to custom debug levels of increasing verbosity
    # @schema
    # type: [string, integer]
    # default: debug
    # @schema
    zapLogLevel: debug
    # String: Zap Level at and above which stacktraces are captured (one of 'info', 'error', 'panic')
    # @schema
    # enum: [info,error,panic]
    # default: error
    # @schema
    # zapStacktraceLevel: error
# String: Zap time encoding (one of 'epoch', 'millis', 'nano', 'iso8601', 'rfc3339' or 'rfc3339nano'). Defaults to 'epoch'
# @schema
# enum: [epoch,millis,nano,iso8601,rfc3339,rfc3339nano]
# default: epoch
# @schema
# zapTimeEncoding: epoch

# @schema
# additionalProperties: true
# @schema
# -- Decode pod configuration
decode:
  create: true
  autoscaling:
    enabled: false
  replicas: 1
  # @schema
  # additionalProperties: true
  # @schema
  nodeSelector: {}
  # schedulerName -- Name of the scheduler to use for scheduling decode pods (overrides global schedulerName)
  # schedulerName: decode-scheduler

  # VLLM parallelism configuration
  parallelism:
    tensor: 1
    data: 1
    dataLocal: 1
    workers: 1
  # accelerator:
  #   # Optional: Override global accelerator configuration for decode pods
  #   # When not specified, falls back to global accelerator settings
  #
  #   # Override accelerator type for decode pods
  #   # Supported: nvidia, intel-i915, intel-xe, intel-gaudi, amd, google, cpu
  #   type: nvidia
  #
  #   # Override DRA (Dynamic Resource Allocation) mode for decode pods
  #   # When not specified, falls back to accelerator.dra
  #   dra: false

  # @schema
  # type: array
  # items:
  #   type: object
  #   required: [name, resourceClaimTemplateName]
  #   properties:
  #     name:
  #       type: string
  #     resourceClaimTemplateName:
  #       type: string
  # @schema
  # Additional resource claims for non-accelerator resources (e.g., IMEX channels)
  # These will be merged with accelerator claims when accelerator.dra is enabled
  resourceClaims: []
  # Example:
  # - name: llm-d-imex-channel-0
  #   resourceClaimTemplateName: llm-d-imex-channel-0

  # @schema
  # type: array
  # items:
  #   type: object
  #   required: [name, resourceClaimTemplateName]
  #   properties:
  #     name:
  #       type: string
  #     resourceClaimTemplateName:
  #       type: string
  # @schema
  # Additional resource claims for non-accelerator resources (e.g., IMEX channels)
  # These will be merged with accelerator claims when accelerator.dra is enabled
  resourceClaims: []
  # Example:
  # - name: llm-d-imex-channel-0
  #   resourceClaimTemplateName: llm-d-imex-channel-0

  # @schema
  # type: array
  # items:
  #   type: object
  #   additionalProperties: true
  # @schema
  containers:
    - name: "vllm"
      image:
        registry: ghcr.m.daocloud.io
        repository: llm-d/llm-d-inference-sim
        tag: latest
      # type of command:
      #   vllmServe - modelservice will add the command vllm serve to the container
      #   imageDefault - no command will be added; the default command defined in the image will be used
      #   custom - use user provided "command"
      modelCommand: imageDefault
      # Required when modelCommand is "custom"
      command: []
      # @schema
      # items:
      #   type: string
      # @schema
      args: []
      # @schema
      # items:
      #   $ref: https://raw.githubusercontent.com/yannh/kubernetes-json-schema/master/master/_definitions.json#/definitions/io.k8s.api.core.v1.EnvVar
      # @schema
      env: []
      imagePullPolicy: ""
      # list of ports exposed by the container
      # @schema
      # items:
      #   $ref: https://raw.githubusercontent.com/yannh/kubernetes-json-schema/master/master/_definitions.json#/definitions/io.k8s.api.core.v1.ContainerPort
      # @schema
      ports: []
      #   - containerPort: 8200  # matches routing.proxy.targetPort, set for metrics scraping with  monitoring.podmonitor.enabled true
      #     name: metrics
      #     protocol: TCP
      # @schema
      # $ref: https://raw.githubusercontent.com/yannh/kubernetes-json-schema/master/master/_definitions.json#/definitions/io.k8s.api.core.v1.ResourceRequirements
      # @schema
      resources:
        # @schema
        # additionalProperties: true
        # @schema
        limits: {}
        # @schema
        # additionalProperties: true
        # @schema
        requests: {}
      # when set, a volumeMount (and volume) is created for model storage
      mountModelVolume: true
      # @schema
      # type: array
      # items:
      #   type: object
      #   additionalProperties: true
      # @schema
      volumeMounts:
        - name: metrics-volume
          mountPath: /.config
  # @schema
  # type: array
  # items:
  #   type: object
  #   additionalProperties: true
  # @schema
  volumes:
    - name: metrics-volume
      emptyDir: {}
  # @schema
  # type: array
  # items:
  #   $ref: https://raw.githubusercontent.com/yannh/kubernetes-json-schema/master/master/_definitions.json#/definitions/io.k8s.api.core.v1.Container
  # @schema
  # Additional init containers to run before the main containers
  # These will be added alongside the routing proxy init container (if enabled)
  initContainers: []
  # hostIPC -- Boolean: Use the host's ipc namespace.
  # -- Not set by default.
  # -- Only an option for LWS (multinode) See: https://github.com/kubernetes-sigs/lws/blob/main/config/crd/bases/leaderworkerset.x-k8s.io_leaderworkersets.yaml#L12196.
  # hostIPC: false

  # hostPID -- Boolean: Use the host's pid namespace.
  # -- Not set by default.
  # -- Only an option for LWS (multinode) See: https://github.com/kubernetes-sigs/lws/blob/main/config/crd/bases/leaderworkerset.x-k8s.io_leaderworkersets.yaml#L12207.
  # hostPID: false

  # enableServiceLinks -- Boolean: Indicates whether information about services should be injected into pod's environment variables.
  # -- Not set by default (defaults to true if not specified).
  # enableServiceLinks: false

  # terminationGracePeriodSeconds -- Duration in seconds the pod needs to terminate gracefully.
  # -- Not set by default (defaults to 30 seconds if not specified).
  # terminationGracePeriodSeconds: 60

  # subGroupPolicy -- object: SubGroupPolicy describes the policy that will be applied when creating subgroups in each replica.
  # -- Not set by default
  # -- Only an option for LWS (multinode) See: https://github.com/kubernetes-sigs/lws/blob/main/config/crd/bases/leaderworkerset.x-k8s.io_leaderworkersets.yaml#L8207
  # subGroupPolicy:
  #   subGroupSize: 8

  # subGroupExclusiveToplogy -- Boolean: Should the `subgroup-exclusive-topology` annotation be added to the LWS
  # -- Not set by default
  # -- Only an option for LWS (multinode)
  # subGroupExclusiveToplogy: true

  # XPU-specific node affinity
  acceleratorTypes:
    labelKey: ""
    # @schema
    # items:
    #   type: string
    # @schema
    labelValues: []
  # @schema
  # additionalProperties: true
  # @schema
  # Monitoring configuration for decode pods
  monitoring:
    # PodMonitor configuration for Prometheus Operator
    podmonitor:
      # enabled -- Create PodMonitor resource for decode deployment
      enabled: false
      # portName -- Port name to scrape metrics from (must match container port name)
      portName: "metrics"
      # path -- HTTP path to scrape metrics from
      path: "/metrics"
      # interval -- Interval at which metrics should be scraped
      interval: "30s"
      # scrapeTimeout -- Timeout after which the scrape is ended
      # scrapeTimeout: "10s"
      # @schema
      # additionalProperties: true
      # @schema
      # labels -- Additional labels to be added to the PodMonitor
      labels: {}
      # annotations -- Additional annotations to be added to the PodMonitor
      # @schema
      # additionalProperties: true
      # @schema
      annotations: {}
      # relabelings -- RelabelConfigs to apply to samples before scraping
      relabelings: []
      # metricRelabelings -- MetricRelabelConfigs to apply to samples before ingestion
      metricRelabelings: []
      # namespaceSelector -- Selector to select which namespaces the Endpoints objects are discovered from
      # namespaceSelector: {}
# @schema
# additionalProperties: true
# @schema
# Prefill pod configuation
prefill:
  create: true
  autoscaling:
    enabled: false
  replicas: 0
  # @schema
  # additionalProperties: true
  # @schema
  nodeSelector: {}
  # schedulerName -- Name of the scheduler to use for scheduling prefill pods (overrides global schedulerName)
  # schedulerName: prefill-scheduler

  # VLLM parallelism configuration
  parallelism:
    tensor: 1
    data: 1
    dataLocal: 1
    workers: 1
  # accelerator:
  #   # Optional: Override global accelerator configuration for prefill pods
  #   # When not specified, falls back to global accelerator settings
  #
  #   # Override accelerator type for prefill pods
  #   # Supported: nvidia, intel-i915, intel-xe, intel-gaudi, amd, google, cpu
  #   type: intel-gaudi
  #
  #   # Override DRA (Dynamic Resource Allocation) mode for prefill pods
  #   # When not specified, falls back to accelerator.dra
  #   dra: false

  # @schema
  # type: array
  # items:
  #   type: object
  #   required: [name, resourceClaimTemplateName]
  #   properties:
  #     name:
  #       type: string
  #     resourceClaimTemplateName:
  #       type: string
  # @schema
  # Additional resource claims for non-accelerator resources (e.g., IMEX channels)
  # These will be merged with accelerator claims when accelerator.dra is enabled
  resourceClaims: []
  # Example:
  # - name: llm-d-imex-channel-0
  #   resourceClaimTemplateName: llm-d-imex-channel-0

  # @schema
  # type: array
  # items:
  #   type: object
  #   required: [name, resourceClaimTemplateName]
  #   properties:
  #     name:
  #       type: string
  #     resourceClaimTemplateName:
  #       type: string
  # @schema
  # Additional resource claims for non-accelerator resources (e.g., IMEX channels)
  # These will be merged with accelerator claims when accelerator.dra is enabled
  resourceClaims: []
  # Example:
  # - name: llm-d-imex-channel-0
  #   resourceClaimTemplateName: llm-d-imex-channel-0

  # @schema
  # type: array
  # items:
  #   type: object
  #   additionalProperties: true
  # @schema
  containers:
    - name: "vllm"
      image:
        registry: ghcr.m.daocloud.io
        repository: llm-d/llm-d-inference-sim
        tag: latest
      modelCommand: imageDefault
      mountModelVolume: true
      # @schema
      # items:
      #   type: string
      # @schema
      args: []
      # @schema
      # items:
      #   $ref: https://raw.githubusercontent.com/yannh/kubernetes-json-schema/master/master/_definitions.json#/definitions/io.k8s.api.core.v1.EnvVar
      # @schema
      env: []
      imagePullPolicy: ""
      command: []
      # list of ports exposed by the container
      # @schema
      # items:
      #   $ref: https://raw.githubusercontent.com/yannh/kubernetes-json-schema/master/master/_definitions.json#/definitions/io.k8s.api.core.v1.ContainerPort
      # @schema
      ports: []
      #   - containerPort: 8000  # matches routing.servicePort, set for metrics scraping with  monitoring.podmonitor.enabled true
      #     name: metrics
      #     protocol: TCP
      # @schema
      # $ref: https://raw.githubusercontent.com/yannh/kubernetes-json-schema/master/master/_definitions.json#/definitions/io.k8s.api.core.v1.ResourceRequirements
      # @schema
      resources:
        # @schema
        # additionalProperties: true
        # @schema
        limits: {}
        # @schema
        # additionalProperties: true
        # @schema
        requests: {}
      # @schema
      # type: array
      # items:
      #   type: object
      #   additionalProperties: true
      # @schema
      volumeMounts:
        - name: metrics-volume
          mountPath: /.config
  # @schema
  # type: array
  # items:
  #   type: object
  #   additionalProperties: true
  # @schema
  volumes:
    - name: metrics-volume
      emptyDir: {}
  # @schema
  # type: array
  # items:
  #   $ref: https://raw.githubusercontent.com/yannh/kubernetes-json-schema/master/master/_definitions.json#/definitions/io.k8s.api.core.v1.Container
  # @schema
  # Additional init containers to run before the main containers
  initContainers: []
  # hostIPC -- Boolean: Use the host's ipc namespace.
  # -- Not set by default.
  # -- Only an option for LWS (multinode) See: https://github.com/kubernetes-sigs/lws/blob/main/config/crd/bases/leaderworkerset.x-k8s.io_leaderworkersets.yaml#L12196.
  # hostIPC: false

  # hostPID -- Boolean: Use the host's pid namespace.
  # -- Not set by default.
  # -- Only an option for LWS (multinode) See: https://github.com/kubernetes-sigs/lws/blob/main/config/crd/bases/leaderworkerset.x-k8s.io_leaderworkersets.yaml#L12207.
  # hostPID: false

  # enableServiceLinks -- Boolean: Indicates whether information about services should be injected into pod's environment variables.
  # -- Not set by default (defaults to true if not specified).
  # enableServiceLinks: false

  # terminationGracePeriodSeconds -- Duration in seconds the pod needs to terminate gracefully.
  # -- Not set by default (defaults to 30 seconds if not specified).
  # terminationGracePeriodSeconds: 60

  # subGroupPolicy -- object: SubGroupPolicy describes the policy that will be applied when creating subgroups in each replica.
  # -- Not set by default
  # -- Only an option for LWS (multinode) See: https://github.com/kubernetes-sigs/lws/blob/main/config/crd/bases/leaderworkerset.x-k8s.io_leaderworkersets.yaml#L8207
  # subGroupPolicy:
  #   subGroupSize: 8

  # subGroupExclusiveToplogy -- Boolean: Should the `subgroup-exclusive-topology` annotation be added to the LWS
  # -- Not set by default
  # -- Only an option for LWS (multinode)
  # subGroupExclusiveToplogy: true

  # XPU-specific node affinity
  acceleratorTypes:
    labelKey: ""
    # @schema
    # items:
    #   type: string
    # @schema
    labelValues: []
  # @schema
  # additionalProperties: true
  # @schema
  # Monitoring configuration for prefill pods
  monitoring:
    # PodMonitor configuration for Prometheus Operator
    podmonitor:
      # enabled -- Create PodMonitor resource for prefill deployment
      enabled: false
      # portName -- Port name to scrape metrics from (must match container port name)
      portName: "metrics"
      # path -- HTTP path to scrape metrics from
      path: "/metrics"
      # interval -- Interval at which metrics should be scraped
      interval: "30s"
      # scrapeTimeout -- Timeout after which the scrape is ended
      # scrapeTimeout: "10s"
      # @schema
      # additionalProperties: true
      # @schema
      # labels -- Additional labels to be added to the PodMonitor
      labels: {}
      # annotations -- Additional annotations to be added to the PodMonitor
      # @schema
      # additionalProperties: true
      # @schema
      annotations: {}
      # relabelings -- RelabelConfigs to apply to samples before scraping
      relabelings: []
      # metricRelabelings -- MetricRelabelConfigs to apply to samples before ingestion
      metricRelabelings: []
      # namespaceSelector -- Selector to select which namespaces the Endpoints objects are discovered from
      # namespaceSelector: {}
# @schema
# items:
#   type: object
# @schema
# -- Extra objects to be deployed alongside the main application
extraObjects: []
# Example:
# extraObjects:
#   - apiVersion: v1
#     kind: ConfigMap
#     metadata:
#       name: example-config
#     data:
#       key: value
