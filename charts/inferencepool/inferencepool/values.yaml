# child values
inferencepool:
  inferenceExtension:
    replicas: 1
    image:
      tag: v1.3.0
      pullPolicy: Always
      registry: k8s.m.daocloud.io
      repository: gateway-api-inference-extension/epp
    extProcPort: 9002
    env: []
    pluginsConfigFile: "default-plugins.yaml"
    # Define additional container ports
    extraContainerPorts: []
    # Define additional service ports
    extraServicePorts: []
    #  extraServicePorts:
    #    - name: http
    #      port: 8081
    #      protocol: TCP
    #      targetPort: 8081

    # This is the plugins configuration file.
    # pluginsCustomConfig:
    #   custom-plugins.yaml: |
    #     apiVersion: inference.networking.x-k8s.io/v1alpha1
    #     kind: EndpointPickerConfig
    #     plugins:
    #     - type: custom-scorer
    #       parameters:
    #         custom-threshold: 64
    #     schedulingProfiles:
    #     - name: default
    #       plugins:
    #       - pluginRef: custom-scorer

    # Example environment variables:
    # env:
    #   ENABLE_EXPERIMENTAL_FEATURE: "true"
    flags:
      # Log verbosity
      v: 1
    affinity: {}
    tolerations: []
    resources:
      requests:
        cpu: 1000m
        memory: 1Gi
      limits:
        cpu: 4000m
        memory: 8Gi
    # Sidecar configuration for EPP
    sidecar:
      enabled: false
    # Monitoring configuration for EPP
    monitoring:
      interval: "10s"
      # Prometheus ServiceMonitor will be created when enabled for EPP metrics collection
      prometheus:
        enabled: false
        auth:
          enabled: true
          # Service account token secret for authentication
          secretName: inference-gateway-sa-metrics-reader-secret
        # additional labels for the ServiceMonitor
        extraLabels: {}
    tracing:
      enabled: false
      otelExporterEndpoint: "http://localhost:4317"
      sampling:
        sampler: "parentbased_traceidratio"
        samplerArg: "0.1"
    # Latency Predictor Configuration
    latencyPredictor:
      enabled: false
      # Training Server Configuration
      trainingServer:
        image:
          hub: path/to/your/docker/repo # NOTE: Update with your Docker repository path for sidecars
          name: latencypredictor-training-server
          tag: v1.3.0
          pullPolicy: Always
        port: 8000
        resources:
          requests:
            cpu: "2000m"
            memory: "4Gi"
          limits:
            cpu: "4000m"
            memory: "8Gi"
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 20
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8000
          initialDelaySeconds: 45
          periodSeconds: 10
        volumeSize: "20Gi"
        config:
          LATENCY_RETRAINING_INTERVAL_SEC: "1"
          LATENCY_MIN_SAMPLES_FOR_RETRAIN: "100"
          LATENCY_TTFT_MODEL_PATH: "/models/ttft.joblib"
          LATENCY_TPOT_MODEL_PATH: "/models/tpot.joblib"
          LATENCY_TTFT_SCALER_PATH: "/models/ttft_scaler.joblib"
          LATENCY_TPOT_SCALER_PATH: "/models/tpot_scaler.joblib"
          LATENCY_MODEL_TYPE: "xgboost"
          LATENCY_MAX_TRAINING_DATA_SIZE_PER_BUCKET: "5000"
          LATENCY_QUANTILE_ALPHA: "0.9"
      # Prediction Server Configuration
      predictionServers:
        count: 10
        startPort: 8001
        image:
          hub: path/to/your/docker/repo # NOTE: Update with your Docker repository path for sidecars
          name: latencypredictor-prediction-server
          tag: v1.3.0
          pullPolicy: Always
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
          limits:
            cpu: "1000m"
            memory: "2Gi"
        livenessProbe:
          httpGet:
            path: /healthz
          initialDelaySeconds: 15
          periodSeconds: 15
        readinessProbe:
          httpGet:
            path: /readyz
          initialDelaySeconds: 10
          periodSeconds: 5
          failureThreshold: 10
        volumeSize: "10Gi"
        config:
          LATENCY_MODEL_TYPE: "xgboost"
          PREDICT_HOST: "0.0.0.0"
          LOCAL_TTFT_MODEL_PATH: "/server_models/ttft.joblib"
          LOCAL_TPOT_MODEL_PATH: "/server_models/tpot.joblib"
          LOCAL_TTFT_SCALER_PATH: "/server_models/ttft_scaler.joblib"
          LOCAL_TPOT_SCALER_PATH: "/server_models/tpot_scaler.joblib"
      # EPP Environment Variables for Latency Predictor
      eppEnv:
        LATENCY_MAX_SAMPLE_SIZE: "10000"
  inferencePool:
    targetPorts:
      - number: 8000
    modelServerType: vllm # vllm, triton-tensorrt-llm
    apiVersion: inference.networking.k8s.io/v1
    # modelServers: # REQUIRED
    #   matchLabels:
    #     app: vllm-llama3-8b-instruct

    # Should only used if apiVersion is inference.networking.x-k8s.io/v1alpha2, 
    # This will soon be deprecated when upstream GW providers support v1, just doing something simple for now.
    targetPortNumber: 8000
    modelServers:
      matchLabels:
        app: inferx
  # Options: ["gke", "istio", "none"]
  provider:
    name: none
    # GKE-specific configuration.
    # This block is only used if name is "gke".
    gke:
      # Set to true if the cluster is an Autopilot cluster.
      autopilot: false
    # Istio-specific configuration.
    # This block is only used if name is "istio".
    istio:
      destinationRule:
        # Provide a way to override the default calculated host
        host: ""
        # Optional: Enables customization of the traffic policy
        trafficPolicy: {}
        # connectionPool:
        #   http:
        #     maxRequestsPerConnection: 256000
  # experimentalHttpRoute section is used to deploy httproute as part of the epp helm chart.
  # this section is temporary and should be extracted to a separate chart.
  experimentalHttpRoute:
    enabled: false # a flag to indicate whether to create the httproute as part of the chart or not.
    inferenceGatewayName: inference-gateway
  # DEPRECATED and will be removed in v1.3. Instead, use `provider.istio.*`.
  istio:
    destinationRule:
      # Provide a way to override the default calculated host
      host: ""
      # Optional: Enables customization of the traffic policy
      trafficPolicy: {}
      # connectionPool:
      #   http:
      #     maxRequestsPerConnection: 256000
